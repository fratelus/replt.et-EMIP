# ğŸ“– ReplET User Guide

A comprehensive guide to using the **Repl.ET: Eye Tracking Replication Template** for reproducible research.

## ğŸš€ Getting Started

### Prerequisites
- Python 3.8+ (recommended: 3.11)
- Git
- Text editor (VS Code, PyCharm, etc.)

### Installation

```bash
# Clone the repository
git clone https://github.com/your-org/ReplET.git
cd ReplET

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r config/requirements.txt
```

## ğŸ“ Understanding the Structure

### Core Components

```
ReplET/
â”œâ”€â”€ ğŸ“„ metadata.json              # Study overview & objectives
â”œâ”€â”€ ğŸ‘¥ participants/              # Demographics & recruitment
â”œâ”€â”€ ğŸ–¥ï¸  equipment/                # Eye tracker & setup specs
â”œâ”€â”€ ğŸ¯ stimuli/                   # Code samples & materials
â”œâ”€â”€ ğŸ“ aois/                      # Areas of Interest definitions
â”œâ”€â”€ ğŸ“‹ collection/                # Data collection protocols
â”œâ”€â”€ ğŸ”§ preprocessing/             # Data cleaning procedures
â”œâ”€â”€ ğŸ“Š analysis/                  # Statistical methods & results
â”œâ”€â”€ âš ï¸  validity/                 # Threats & limitations
â”œâ”€â”€ ğŸ”„ reproducibility/           # Replication materials
â”œâ”€â”€ ğŸ“ schemas/                   # JSON validation schemas (13 files)
â”œâ”€â”€ ğŸ§° utils/                     # Assessment and scoring tools
â””â”€â”€ ğŸ—ï¸  data/                     # Organized data storage structure
```

### Example Implementations

- **ğŸ“‹ Template (Root)**: Clean starting template (5.0% score - empty foundation)
- **ğŸ“š examples/basic/**: Learning example (~30% score - basic implementation) 
- **ğŸ† examples/advanced/**: Publication-ready study (83.0% score - complete documentation)

## ğŸ¯ Choosing Your Starting Point

### For Learning â†’ examples/basic/
```bash
cd examples/basic/
python ../../utils/update_readme_with_assessment.py
# Score: ~30% | Status: Basic Setup
```

### For New Study â†’ Root Template
```bash
# Start from clean template
python utils/update_readme_with_assessment.py
# Score: 5.0% | Status: Empty Template
```

### For Publication â†’ examples/advanced/
```bash
cd examples/advanced/
python ../../utils/update_readme_with_assessment.py  
# Score: 83.0% | Status: Complete (Publication Ready)
```

## ğŸ“Š Assessment and Scoring

### Generate Your Study Score
```bash
# From any directory containing study files
python utils/update_readme_with_assessment.py
```

This generates:
- **ğŸ“ˆ Spider graph PNG**: Visual reproducibility assessment
- **ğŸ“‹ Compliance checklist**: Component-by-component status
- **ğŸ“Š Scores table**: Detailed breakdown by criteria
- **ğŸ“ Updated README**: Automatic documentation updates

### Understanding Scores

| Score Range | Status | Meaning |
|------------|--------|---------|
| **80-100%** | âœ… Complete | Publication-ready documentation |
| **60-79%** | âš ï¸ Good Practice | Solid foundation, minor gaps |
| **40-59%** | ğŸŸ  Needs Work | Basic structure, major improvements needed |
| **0-39%** | âšª Template/Learning | Starting point or educational baseline |

## ğŸ”„ Workflow: From Template to Publication

### Step 1: Initialize Your Study
```bash
# Copy template structure
cp -r ReplET/ MyStudy/
cd MyStudy/

# Run initial assessment
python utils/update_readme_with_assessment.py
# Expected: ~5% (Empty Template)
```

### Step 2: Fill Core Components
Edit these files first:
1. `metadata.json` - Study title, objectives, authors
2. `participants/participants.json` - Demographics and recruitment
3. `equipment/tracker_specs.json` - Your eye tracker setup

```bash
# Check progress
python utils/update_readme_with_assessment.py
# Expected: 20-40% (Basic Setup)
```

### Step 3: Complete Methodology
4. `stimuli/stimuli_metadata.json` - Your experimental materials
5. `aois/aois_definition.json` - Areas of interest
6. `collection/protocol.json` - Data collection procedures
7. `preprocessing/preprocessing.json` - Analysis pipeline

```bash
# Check progress  
python utils/update_readme_with_assessment.py
# Expected: 50-70% (Good Practice)
```

### Step 4: Research Rigor
8. `analysis/analysis.json` - Statistical methods and results
9. `validity/validity.json` - Threats and limitations
10. `reproducibility/reproducibility.json` - Sharing materials

```bash
# Final assessment
python utils/update_readme_with_assessment.py
# Target: 80%+ (Publication Ready)
```

## ğŸ† Advanced Features

### Data Organization
The template includes a complete data organization structure:

```
data/
â”œâ”€â”€ raw/                   # Original data files (not in Git)
â”‚   â”œâ”€â”€ eye_tracking/     # .gazedata, .tsv files
â”‚   â”œâ”€â”€ eeg/              # .eeg, .vhdr, .vmrk files  
â”‚   â””â”€â”€ behavioral/       # Response data, questionnaires
â”œâ”€â”€ processed/            # Cleaned data ready for analysis
â”œâ”€â”€ analysis/             # Final datasets for statistics
â””â”€â”€ results/              # Outputs, figures, reports
```

### Multi-Modal Studies
The advanced example demonstrates:
- **Synchronized data collection**: Eye tracking + EEG + physiological
- **Quality control**: Real-time monitoring and validation
- **Complex analyses**: Machine learning, network analysis, cognitive load

### Validation and Testing
```bash
# Run template validation
python tests/run_tests.py

# Check data integrity
python tests/test_data_integrity.py

# Validate JSON schemas
python utils/validate_jsons.py
```

## ğŸ“ Learning Path

### Beginner: Understand Structure
1. Read `examples/basic/` to see simple implementations
2. Run assessment to see component scoring
3. Practice editing one component at a time

### Intermediate: Build Complete Study  
1. Start from root template
2. Fill components systematically
3. Aim for 60%+ score with good practices

### Advanced: Publication Standards
1. Study `examples/advanced/` for comprehensive documentation
2. Implement advanced features (multi-modal, international collaboration)
3. Achieve 80%+ score for publication readiness

## ğŸ”§ Customization

### Adapting for Your Domain
- Modify `schemas/*.schema.json` for domain-specific requirements
- Customize scoring weights in `utils/update_readme_with_assessment.py`
- Add new components following existing patterns

### Integration with Analysis Tools
```python
# Import scoring functions
from utils.repl_et_score import calculate_reproducibility_score

# Use in your analysis pipeline
score = calculate_reproducibility_score('path/to/study/')
```

## â“ Troubleshooting

### Common Issues

**Low scores despite complete files**:
- Check for template placeholder text
- Ensure realistic, specific content
- Verify file structure matches schemas

**Assessment script errors**:
- Confirm all required JSON files exist
- Validate JSON syntax
- Check Python dependencies

**Data organization confusion**:
- Use provided `data/` structure
- Follow naming conventions
- Keep raw data separate from processed

### Getting Help
- ğŸ“– Check documentation in `.project/docs/`
- ğŸ› Report issues on GitHub
- ğŸ’¬ Join community discussions
- ğŸ“§ Contact maintainers for research collaborations

## ğŸ“ˆ Success Metrics

Track your progress toward reproducible research:

- **ğŸ“‹ Documentation Completeness**: All 10 components filled
- **ğŸ“Š Reproducibility Score**: Target 80%+ for publication
- **ğŸ”„ Replication Support**: Materials available for others
- **âœ… Validation Passing**: All tests and schema validation
- **ğŸŒŸ Community Impact**: Citations and reuse by other researchers

Remember: The goal isn't just high scores, but **genuinely reproducible research** that advances scientific knowledge! 